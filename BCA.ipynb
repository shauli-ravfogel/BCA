{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encodings.bert-base.250k.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100000, 768]), torch.Size([768, 30522]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = np.array([d[\"vec_batch_norm\"] for d in data])\n",
    "with open(\"bert_embeddings.pickle\", \"rb\") as f:\n",
    "    W = pickle.load(f)\n",
    "    \n",
    "H,W = torch.from_numpy(H).double(), torch.from_numpy(W).double()\n",
    "H = H[:100000]\n",
    "W = W.T\n",
    "H.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5134, 0.5406, 0.4952,  ..., 0.4864, 0.3009, 0.5797])\n"
     ]
    }
   ],
   "source": [
    "# N = 100000\n",
    "# d = 128\n",
    "# l = 25\n",
    "# H = (torch.randn(N,d) + torch.rand(N,d)**2)\n",
    "\n",
    "# H = H - torch.mean(H, dim = 0, keepdim = True)\n",
    "# H = H\n",
    "# W = torch.randn(d,l)\n",
    "cov_H = torch.tensor(np.cov(H.detach().cpu().numpy(), rowvar = False))\n",
    "print(cov_H[0]@W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# Q = cov_H[:1000]@W\n",
    "# print(Q)\n",
    "# print(cov_H.shape, W.shape)\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = torch.nn.Parameter(u)\n",
    "# optimizer = torch.optim.SGD([u], lr=0.001, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov_output_projected(u, cov_H, W):\n",
    "    \n",
    "    u_normed = u / torch.norm(u)\n",
    "    P = torch.eye(cov_H.shape[0]) - (u_normed@u_normed.T)\n",
    "    #P = u_normed@u_normed.T\n",
    "    print(P.shape, W.shape, cov_H.shape, u_normed.shape)\n",
    "    first = P@W\n",
    "    print(\"done first\")\n",
    "    second = cov_H@first\n",
    "    print(\"done second\")\n",
    "    third = P@second\n",
    "    print(\"done third\")\n",
    "    fourth = W.T@third\n",
    "    print(\"done fourth\")\n",
    "    return fourth\n",
    "    #return W.T@P@cov_H@P@W\n",
    "\n",
    "def get_cov_output_total(H,W):\n",
    "    with torch.no_grad():\n",
    "        Y_hat = H[:5000]@W \n",
    "        return torch.sum(Y_hat*Y_hat)\n",
    "    #return torch.tensor(np.cov(Y_hat.detach().cpu().numpy(), rowvar = False))\n",
    "\n",
    "def get_loss_func(cov_output_projected):\n",
    "    \n",
    "    loss =  torch.sum(torch.diag(cov_output_projected))\n",
    "    print(\"done loss calculation\")\n",
    "    return loss\n",
    "\n",
    "def get_loss_func2(u, cov_H, W):\n",
    "\n",
    "    u_normed = u / torch.norm(u)\n",
    "    P = torch.eye(cov_H.shape[0]) - (u_normed@u_normed.T)\n",
    "    #P = u_normed@u_normed.T\n",
    "    first = P@W\n",
    "    second = cov_H@first\n",
    "    third = P@second\n",
    "    fourth = torch.sum(W*third)\n",
    "    return fourth\n",
    "\n",
    "def get_projection_to_intersection_of_nullspaces(rowspace_projection_matrices: List[np.ndarray], input_dim: int):\n",
    "    \"\"\"\n",
    "    Given a list of rowspace projection matrices P_R(w_1), ..., P_R(w_n),\n",
    "    this function calculates the projection to the intersection of all nullspasces of the matrices w_1, ..., w_n.\n",
    "    uses the intersection-projection formula of Ben-Israel 2013 http://benisrael.net/BEN-ISRAEL-NOV-30-13.pdf:\n",
    "    N(w1)∩ N(w2) ∩ ... ∩ N(wn) = N(P_R(w1) + P_R(w2) + ... + P_R(wn))\n",
    "    :param rowspace_projection_matrices: List[np.array], a list of rowspace projections\n",
    "    :param dim: input dim\n",
    "    \"\"\"\n",
    "\n",
    "    I = np.eye(input_dim)\n",
    "    Q = np.sum(rowspace_projection_matrices, axis = 0)\n",
    "    P = I - get_rowspace_projection(Q)\n",
    "\n",
    "    return P\n",
    "\n",
    "def get_rowspace_projection(W: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param W: the matrix over its nullspace to project\n",
    "    :return: the projection matrix over the rowspace\n",
    "    \"\"\"\n",
    "\n",
    "    if np.allclose(W, 0):\n",
    "        w_basis = np.zeros_like(W.T)\n",
    "    else:\n",
    "        w_basis = scipy.linalg.orth(W.T) # orthogonal basis\n",
    "\n",
    "    P_W = w_basis.dot(w_basis.T) # orthogonal projection on W's rowspace\n",
    "\n",
    "    return P_W\n",
    "\n",
    "def get_first_pca(H):\n",
    "    pca = PCA(n_components = 1)\n",
    "    pca.fit(H)\n",
    "    return torch.from_numpy(pca.components_.T)\n",
    "\n",
    "def BCA(H,W,n_components, eps = 1e-8, max_iters = 1000, init_pca = True):\n",
    "    \n",
    "    P_nullspace = torch.eye(H.shape[1])\n",
    "    results = []\n",
    "    #cov_out_total = get_cov_output_total(H,W)\n",
    "    total_var_orig = get_cov_output_total(H,W) #get_loss_func(cov_out_total).detach().cpu().numpy().item()\n",
    "    remaining_var = total_var_orig\n",
    "    print(\"Total var original: \", remaining_var)\n",
    "    H_proj = H.clone()\n",
    "    rowspace_projs = []\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        \n",
    "        H_proj = H@P_nullspace # remove previous component \n",
    "        cov_H = torch.from_numpy(np.cov(H_proj.detach().cpu().numpy(), rowvar = False))\n",
    "        \n",
    "        if init_pca:\n",
    "            u = get_first_pca(H_proj.detach().cpu().numpy())\n",
    "        else:\n",
    "            u = torch.randn(H_proj.shape[1], 1)\n",
    "        u = torch.nn.Parameter(u)\n",
    "        optimizer = torch.optim.SGD([u], lr=1e-4, momentum=0.8)\n",
    "        #optimizer = torch.optim.Adam([u])\n",
    "        \n",
    "        diff = 10\n",
    "        j = 0\n",
    "        loss_vals = [np.inf]\n",
    "        patience = 4\n",
    "        patience_counter = 0\n",
    "        \n",
    "        while j < max_iters and patience_counter < patience:\n",
    "            optimizer.zero_grad()\n",
    "            #cov_out = get_cov_output_projected(u,cov_H,W)\n",
    "            #loss = get_loss_func(cov_out)\n",
    "            loss = get_loss_func2(u, cov_H, W)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_vals.append(loss.detach().cpu().numpy().item())\n",
    "            diff = np.abs(loss_vals[-1] - loss_vals[-2])\n",
    "            \n",
    "            if diff > eps:\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if j % 25 == 0: print(\"j, loss, \", j, loss.detach().cpu().numpy().item(), diff)\n",
    "            j += 1\n",
    "        print(\"finished after {} iters\".format(j))\n",
    "        \n",
    "        # calculate new nullspace projection to neutralzie component u\n",
    "        \n",
    "        u_normed = u / torch.norm(u)\n",
    "        rowspace_projs.append((u_normed@u_normed.T).detach().cpu().numpy())\n",
    "        P_nullspace = torch.from_numpy(get_projection_to_intersection_of_nullspaces(rowspace_projs,cov_H.shape[0]))\n",
    "        #P_nullspace = torch.eye(H_proj.shape[1]).double() - u_normed@u_normed.T\n",
    "        \n",
    "        # calcualte explained variance\n",
    "        #cov_out_total = get_cov_output_total(H,W)\n",
    "        total_var = get_cov_output_total(H,W)\n",
    "        #cov_out_projected = get_cov_output_projected(u,cov_H,W)\n",
    "        #total_var_projected = get_loss_func(cov_out_projected).detach().cpu().numpy().item()\n",
    "        total_var_projected = remaining_var - get_loss_func2(u,cov_H,W)\n",
    "        explained_var = total_var_projected / total_var\n",
    "        remaining_var = remaining_var - total_var_projected\n",
    "        \n",
    "        #u = u / u.norm()\n",
    "        results.append({\"vec\": u.squeeze().detach().cpu().numpy(), \"projected_var\": total_var_projected,\n",
    "                       \"total_var\": total_var, \"explained_var\": total_var_projected*100/total_var,\n",
    "                       \"cov_out\":cov_out_projected})\n",
    "    \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total var original:  tensor(3.4003e+09)\n",
      "j, loss,  0 155768.90582658348 inf\n",
      "j, loss,  2 148063.25288920783 13594.808557405311\n",
      "j, loss,  4 142304.65481574967 5933.130929292238\n",
      "j, loss,  6 138352.90200163357 774.957252429449\n",
      "j, loss,  8 137305.78105158376 717.4958084040263\n",
      "j, loss,  10 135306.61000177864 985.1884706384444\n",
      "j, loss,  12 133918.25313571922 604.7058947051992\n",
      "j, loss,  14 132875.15765970555 517.2771117776283\n",
      "j, loss,  16 131854.25553355517 500.9627363670443\n",
      "j, loss,  18 130973.92237513408 418.5667270269478\n",
      "j, loss,  20 130232.62488413988 357.93383542636\n",
      "j, loss,  22 129576.05019058849 318.69090012906236\n",
      "j, loss,  24 128998.81754779919 278.5579849306232\n",
      "j, loss,  26 128496.3249894953 242.80060436206986\n",
      "j, loss,  28 128054.33897504794 214.19489677906677\n",
      "j, loss,  30 127662.79414672998 189.92334473092342\n",
      "j, loss,  32 127314.43890327595 169.23853685667564\n",
      "j, loss,  34 127002.05235633712 152.1201874371909\n",
      "j, loss,  36 126719.4177254966 137.9319589875522\n",
      "j, loss,  38 126461.55297522279 126.11460507797892\n",
      "j, loss,  40 126224.24183542596 116.32469296071213\n",
      "j, loss,  42 126003.92377097017 108.231344056956\n",
      "j, loss,  44 125797.67924094536 101.52332348148047\n",
      "j, loss,  46 125603.12119064471 95.94968756689923\n",
      "j, loss,  48 125418.29335911458 91.30505642277421\n",
      "j, loss,  50 125241.60063658634 87.41678429096646\n",
      "j, loss,  52 125071.74711568191 84.1427387168369\n",
      "j, loss,  54 124907.68155728403 81.36668111971812\n",
      "j, loss,  56 124748.55387824187 78.99289417960972\n",
      "j, loss,  58 124593.67929762576 76.942867667065\n",
      "j, loss,  60 124442.50763589637 75.15283296148118\n",
      "j, loss,  62 124294.59750881785 73.57131687001674\n",
      "j, loss,  64 124149.59511809406 72.15691588910704\n",
      "j, loss,  66 124007.21696385338 70.87642337816942\n",
      "j, loss,  68 123867.23582173833 69.70327391942556\n",
      "j, loss,  70 123729.46939635364 68.61627564279479\n",
      "j, loss,  72 123593.77112791009 67.59859290496388\n",
      "j, loss,  74 123460.02273912408 66.63692024404008\n",
      "j, loss,  76 123328.12821672286 65.72079817355552\n",
      "j, loss,  78 123198.00898789376 64.84204300573037\n",
      "j, loss,  80 123069.60008760498 63.99427527197986\n",
      "j, loss,  82 122942.84713784556 63.172534655139316\n",
      "j, loss,  84 122817.70398363168 62.372969490636024\n",
      "j, loss,  86 122694.1308549542 61.5925886262994\n",
      "j, loss,  88 122572.09294744108 60.82906408330018\n",
      "j, loss,  90 122451.55933540322 60.08057449494663\n",
      "j, loss,  92 122332.50214790479 59.34568136143207\n",
      "j, loss,  94 122214.89595174511 58.623231905789\n",
      "j, loss,  96 122098.71729559862 57.91228364482231\n",
      "j, loss,  98 121983.94437803025 57.212046645596274\n",
      "j, loss,  100 121870.55680919359 56.52184005176241\n",
      "j, loss,  102 121758.53544210532 55.84105996982544\n",
      "j, loss,  104 121647.86225447818 55.169156283314805\n",
      "j, loss,  106 121538.52026634425 54.505616372829536\n",
      "j, loss,  108 121430.49348208928 53.84995414885634\n",
      "j, loss,  110 121323.76684823199 53.20170311136462\n",
      "j, loss,  112 121218.32622043455 52.56041244402877\n",
      "j, loss,  114 121114.1583349056 51.92564532421238\n",
      "j, loss,  116 121011.25078067367 51.296978858386865\n",
      "j, loss,  118 120909.59197028843 50.67400512122549\n",
      "j, loss,  120 120809.17110727349 50.056332942360314\n",
      "j, loss,  122 120709.97814933908 49.44359011783672\n",
      "j, loss,  124 120612.00376680639 48.835425847486476\n",
      "j, loss,  126 120515.23929609965 48.23151320073521\n",
      "j, loss,  128 120419.67668841456 47.631551498547196\n",
      "j, loss,  130 120325.30845389407 47.03526850769413\n",
      "j, loss,  132 120232.12760179571 46.44242238003062\n",
      "j, loss,  134 120140.12757725522 45.85280326777138\n",
      "j, loss,  136 120049.30219529849 45.266234616123256\n",
      "j, loss,  138 119959.64557286084 44.68257406624616\n",
      "j, loss,  140 119871.15205954548 44.10171398532111\n",
      "j, loss,  142 119783.81616792144 43.523581602887134\n",
      "j, loss,  144 119697.63250410766 42.948138772684615\n",
      "j, loss,  146 119612.59569942264 42.375381345904316\n",
      "j, loss,  148 119528.70034380718 41.805338191756164\n",
      "j, loss,  150 119445.94092172089 41.23806988475553\n",
      "j, loss,  152 119364.31175115728 40.673667063631\n",
      "j, loss,  154 119283.80692635362 40.112248514502426\n",
      "j, loss,  156 119204.42026472944 39.553958996140864\n",
      "j, loss,  158 119126.14525850507 38.99896684719715\n",
      "j, loss,  160 119048.97503138662 38.447461410367396\n",
      "j, loss,  162 118972.90230062848 37.89965030635358\n",
      "j, loss,  164 118897.91934469792 37.35575661408075\n",
      "j, loss,  166 118824.01797670961 36.81601596662949\n",
      "j, loss,  168 118751.1895236939 36.280673631321406\n",
      "j, loss,  170 118679.4248117171 35.74998158670496\n",
      "j, loss,  172 118608.71415677067 35.22419565235032\n",
      "j, loss,  174 118539.04736130791 34.703572685059044\n",
      "j, loss,  176 118470.41371622014 34.188367887807544\n",
      "j, loss,  178 118402.80200800169 33.678832250472624\n",
      "j, loss,  180 118336.20053079688 33.175210150002385\n",
      "j, loss,  182 118270.59710298055 32.67773713100178\n",
      "finished after 183 iters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'total_var_projected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-cdeba47ba440>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-4804ae3aceda>\u001b[0m in \u001b[0;36mBCA\u001b[0;34m(H, W, n_components, eps, max_iters, init_pca)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m#total_var_projected = get_loss_func(cov_out_projected).detach().cpu().numpy().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mtotal_Var_projected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_func2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcov_H\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mexplained_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_var\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtotal_var_projected\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mremaining_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_var\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtotal_var_projected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_var_projected' is not defined"
     ]
    }
   ],
   "source": [
    "bca = BCA(H,W,n_components=1, eps = 35, init_pca = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bca[0][\"vec\"].T@bca[3][\"vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vecs = np.array([x[\"vec\"] for x in bca])\n",
    "for i,v in enumerate(vecs):\n",
    "    for j, v2 in enumerate(vecs):\n",
    "        if j <= i: continue\n",
    "            \n",
    "        print(i,j, v@v2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for i in range(len(bca)):\n",
    "    \n",
    "    #print(bca[i][\"explained_var\"])\n",
    "    \n",
    "cov_out_total = get_cov_output_total(H,W)\n",
    "total_var = get_loss_func(cov_out_total).detach().cpu().numpy().item()    \n",
    "vars = [x[\"explained_var\"] for x in bca]\n",
    "print(vars)\n",
    "print(sum(vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bca[-1][\"cov_out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bca[-3][\"explained_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,v = np.random.randn(768), np.random.randn(768).T\n",
    "u,v = u / np.linalg.norm(u), v/np.linalg.norm(v)\n",
    "u@v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-12:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(768,32000)\n",
    "H = np.random.rand(10000,768)\n",
    "H*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(768,32000)\n",
    "H = torch.randn(10000, 768)\n",
    "Y_hat = H[:10000]@W \n",
    "torch.mean(Y_hat*Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_hat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
