{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import tqdm\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encodings.bert-base.250k.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data = [d for d in data if d[\"gold\"].lower() not in [\"is\", \"are\", \"have\", \"had\", \"has\",\"then\",\"or\",\"and\",\",\",\n",
    "                                                    \".\",\"-\",\"'\",\"of\",\"in\",\"on\",\"off\",\"the\",\"a\",\"an\", \"be\", \"been\", \"from\", \"to\",\n",
    "                                                    \"about\", \"around\", \"near\", \"it\", \"he\", \"she\", \"her\",\"him\",\"their\", \"they\", \"i\", \"was\", \"were\",\"them\",\n",
    "                                                    \"than\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = np.array([d[\"sent\"] for d in data])\n",
    "masked_tokens = np.array([d[\"gold\"] for d in data])\n",
    "top_words = np.array([d[\"top_words\"][0] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omid released the instrumental album , Afterwords 3 , on Alpha Pup Records in 2007 .\n"
     ]
    }
   ],
   "source": [
    "print(sents[3])\n",
    "# print(masked_tokens[2])\n",
    "# print(top_words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([150000, 768]), torch.Size([768, 30522]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = np.array([d[\"vec_batch_norm\"] for d in data])\n",
    "with open(\"bert_embeddings.pickle\", \"rb\") as f:\n",
    "    W = pickle.load(f)\n",
    "    \n",
    "H,W = torch.from_numpy(H).to(device), torch.from_numpy(W).to(device)\n",
    "H = H[:150000]\n",
    "W = W.T\n",
    "H.shape, W.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.device, H.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 100000\n",
    "# d = 128\n",
    "# l = 25\n",
    "# H = (torch.randn(N,d) + torch.rand(N,d)**2)\n",
    "\n",
    "# H = H - torch.mean(H, dim = 0, keepdim = True)\n",
    "# H = H\n",
    "# W = torch.randn(d,l)\n",
    "cov_H = torch.tensor(np.cov(H.detach().cpu().numpy(), rowvar = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_H = cov_H.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# Q = cov_H[:1000]@W\n",
    "# print(Q)\n",
    "# print(cov_H.shape, W.shape)\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = torch.nn.Parameter(u)\n",
    "# optimizer = torch.optim.SGD([u], lr=0.001, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov_output_projected(u, cov_H, W):\n",
    "    \n",
    "    u_normed = u / torch.norm(u)\n",
    "    P = torch.eye(cov_H.shape[0]) - (u_normed@u_normed.T)\n",
    "    #P = u_normed@u_normed.T\n",
    "    print(P.shape, W.shape, cov_H.shape, u_normed.shape)\n",
    "    first = P@W\n",
    "    print(\"done first\")\n",
    "    second = cov_H@first\n",
    "    print(\"done second\")\n",
    "    third = P@second\n",
    "    print(\"done third\")\n",
    "    fourth = W.T@third\n",
    "    print(\"done fourth\")\n",
    "    return fourth\n",
    "    #return W.T@P@cov_H@P@W\n",
    "\n",
    "def get_cov_output_total(H,W,n=10000):\n",
    "    with torch.no_grad():\n",
    "        Y_hat = H[:n]@W \n",
    "        Y_hat = Y_hat - torch.mean(Y_hat, dim = 1, keepdim = True)\n",
    "        return torch.sum(Y_hat*Y_hat)/Y_hat.shape[0]\n",
    "    #return torch.tensor(np.cov(Y_hat.detach().cpu().numpy(), rowvar = False))\n",
    "\n",
    "def eval_total_var(H,W):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        k = 1000\n",
    "        Y_sum = torch.zeros(W.shape[1]).to(device)\n",
    "        Y_sqr_sum = torch.zeros(W.shape[1]).to(device)\n",
    "    \n",
    "        for i in range(0,len(H), k):\n",
    "            Y = H[i:i+k]@W\n",
    "            Y_sum += torch.sum(Y, dim = 0)\n",
    "            Y_sqr_sum += torch.sum(Y**2, dim = 0)\n",
    "    \n",
    "        Y_sqr_sum /= len(H)\n",
    "        Y_sum /= len(H)\n",
    "        return (Y_sqr_sum - Y_sum**2).mean().detach().cpu().numpy().item() # mean over vocab\n",
    "    \n",
    "def get_loss_func(cov_output_projected):\n",
    "    \n",
    "    loss =  torch.sum(torch.diag(cov_output_projected))\n",
    "    print(\"done loss calculation\")\n",
    "    return loss\n",
    "\n",
    "def get_loss_func2(u, cov_H, W):\n",
    "\n",
    "    u_normed = u / torch.norm(u)\n",
    "    P = torch.eye(cov_H.shape[0]) - (u_normed@u_normed.T)\n",
    "    #print(P.dtype, W.dtype, cov_H.dtype)\n",
    "    #P = u_normed@u_normed.T\n",
    "    first = P@W\n",
    "    second = cov_H@first\n",
    "    third = P@second\n",
    "    fourth = torch.sum(W*third)/third.shape[1] # mean over vocab\n",
    "    return fourth \n",
    "\n",
    "def get_projection_to_intersection_of_nullspaces(rowspace_projection_matrices: List[np.ndarray], input_dim: int):\n",
    "    \"\"\"\n",
    "    Given a list of rowspace projection matrices P_R(w_1), ..., P_R(w_n),\n",
    "    this function calculates the projection to the intersection of all nullspasces of the matrices w_1, ..., w_n.\n",
    "    uses the intersection-projection formula of Ben-Israel 2013 http://benisrael.net/BEN-ISRAEL-NOV-30-13.pdf:\n",
    "    N(w1)∩ N(w2) ∩ ... ∩ N(wn) = N(P_R(w1) + P_R(w2) + ... + P_R(wn))\n",
    "    :param rowspace_projection_matrices: List[np.array], a list of rowspace projections\n",
    "    :param dim: input dim\n",
    "    \"\"\"\n",
    "\n",
    "    I = np.eye(input_dim)\n",
    "    Q = np.sum(rowspace_projection_matrices, axis = 0)\n",
    "    P = I - get_rowspace_projection(Q)\n",
    "\n",
    "    return P\n",
    "\n",
    "def get_rowspace_projection(W: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param W: the matrix over its nullspace to project\n",
    "    :return: the projection matrix over the rowspace\n",
    "    \"\"\"\n",
    "\n",
    "    if np.allclose(W, 0):\n",
    "        w_basis = np.zeros_like(W.T)\n",
    "    else:\n",
    "        w_basis = scipy.linalg.orth(W.T) # orthogonal basis\n",
    "\n",
    "    P_W = w_basis.dot(w_basis.T) # orthogonal projection on W's rowspace\n",
    "\n",
    "    return P_W\n",
    "\n",
    "def get_first_pca(H):\n",
    "    pca = PCA(n_components = 1)\n",
    "    pca.fit(H)\n",
    "    return torch.from_numpy(pca.components_.T)\n",
    "\n",
    "def BCA(H,W,n_components, eps = 1e-8, max_iters = 1500, min_iters = 200, init_pca = True):\n",
    "    \n",
    "    P_nullspace = torch.eye(H.shape[1])\n",
    "    results = []\n",
    "    #cov_out_total = get_cov_output_total(H,W)\n",
    "    total_var_orig = eval_total_var(H,W) #get_loss_func(cov_out_total).detach().cpu().numpy().item()\n",
    "    remaining_var = total_var_orig\n",
    "    print(\"Total var original: \", remaining_var)\n",
    "    H_proj = H.clone()\n",
    "    rowspace_projs = []\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        print(\"****\" + \"ITER {}\".format(i) + \"****\")\n",
    "        H_proj = H@P_nullspace # remove previous component \n",
    "        cov_H = torch.from_numpy(np.cov(H_proj.detach().cpu().numpy(), rowvar = False)).float()\n",
    "        \n",
    "        if init_pca:\n",
    "            u = get_first_pca(H_proj.detach().cpu().numpy())\n",
    "        else:\n",
    "            u = 1e-2*torch.randn(H_proj.shape[1], 1)\n",
    "        u = torch.nn.Parameter(u)\n",
    "        #optimizer = torch.optim.SGD([u], lr=1)\n",
    "        optimizer = torch.optim.Adam([u])\n",
    "        \n",
    "        diff = 10\n",
    "        j = 0\n",
    "        loss_vals = [np.inf]\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        while j < max_iters and patience_counter < patience:\n",
    "            optimizer.zero_grad()\n",
    "            #cov_out = get_cov_output_projected(u,cov_H,W)\n",
    "            #loss = get_loss_func(cov_out)\n",
    "            loss = get_loss_func2(u, cov_H, W)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_vals.append(loss.detach().cpu().numpy().item())\n",
    "            diff = np.abs(loss_vals[-1] - loss_vals[-2])\n",
    "            \n",
    "            if diff > eps:\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                if j > min_iters:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "            if j % 50 == 0: print(\"j, loss, \", j, loss.detach().cpu().numpy().item(), diff)\n",
    "            j += 1\n",
    "        print(\"finished after {} iters\".format(j))\n",
    "        \n",
    "        # calculate new nullspace projection to neutralzie component u\n",
    "        \n",
    "        u_normed = u / torch.norm(u)\n",
    "        rowspace_projs.append((u_normed@u_normed.T).detach().cpu().numpy())\n",
    "        P_nullspace = torch.from_numpy(get_projection_to_intersection_of_nullspaces(rowspace_projs,cov_H.shape[0])).float()\n",
    "        #P_nullspace = torch.eye(H_proj.shape[1]).double() - u_normed@u_normed.T\n",
    "        \n",
    "        # calcualte explained variance\n",
    "        #cov_out_total = get_cov_output_total(H,W)\n",
    "        total_var = eval_total_var(H,W)\n",
    "        #cov_out_projected = get_cov_output_projected(u,cov_H,W)\n",
    "        #total_var_projected = get_loss_func(cov_out_projected).detach().cpu().numpy().item()\n",
    "        total_var_projected = remaining_var - get_loss_func2(u,cov_H,W).detach().cpu().numpy().item()\n",
    "        explained_var = total_var_projected / total_var_orig\n",
    "        remaining_var = remaining_var - total_var_projected\n",
    "        \n",
    "        u = u / u.norm()\n",
    "        results.append({\"vec\": u.squeeze().detach().cpu().numpy(), \"projected_var\": total_var_projected,\n",
    "                       \"total_var\": total_var, \"explained_var\": explained_var*100})\n",
    "        \n",
    "        with open(\"bca.pickle\", \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "            \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total var original:  6.112844467163086\n",
      "****ITER 0****\n",
      "j, loss,  0 5.748241424560547 inf\n",
      "j, loss,  50 4.944725513458252 0.01856088638305664\n",
      "j, loss,  100 4.428926467895508 0.004486083984375\n",
      "j, loss,  150 4.293493270874023 0.0017309188842773438\n",
      "j, loss,  200 4.229542255401611 0.0009198188781738281\n",
      "j, loss,  250 4.197164535522461 0.000438690185546875\n",
      "j, loss,  300 4.181656837463379 0.00021791458129882812\n",
      "j, loss,  350 4.173521041870117 0.00012159347534179688\n",
      "j, loss,  400 4.168801784515381 7.486343383789062e-05\n",
      "j, loss,  450 4.165884971618652 4.5299530029296875e-05\n",
      "finished after 453 iters\n"
     ]
    }
   ],
   "source": [
    "bca = BCA(H,W,n_components=1, eps = 0.5*1e-4, init_pca = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bca.pickle\", \"wb\") as f:\n",
    "    pickle.dump(bca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bca.pickle\", \"rb\") as f:\n",
    "    bca = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vecs = np.array([x[\"vec\"] for x in bca])\n",
    "for i,v in enumerate(vecs):\n",
    "    for j, v2 in enumerate(vecs):\n",
    "        if j <= i: continue\n",
    "            \n",
    "        print(i,j, v@v2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "=========================\n",
      "14.293095243475268\n",
      "1.8796024192443417\n",
      "0.19602017222066867\n",
      "0.03468219526766493\n",
      "0.007907824977824478\n",
      "0.0018865601779827132\n",
      "0.0004523839528763313\n",
      "0.00010860775992305265\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as optimize\n",
    "\n",
    "def f(u,W,V, orthogonal_constraints = []):\n",
    "    norm = np.linalg.norm\n",
    "    grad =  0.5 * ( (norm(u)**2/norm(V.T@u)**2)*V@V.T@u  + (norm(u)**2/norm(W.T@u)**2)*W@W.T@u)\n",
    "    c_orth = 1\n",
    "    for v in orthogonal_constraints:\n",
    "        grad += -c_orth*(1/(u.T@v)**2)*v\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def f2(u,W,V,cov):\n",
    "    \n",
    "    S = cov\n",
    "    t_0 = (u).dot(u)\n",
    "    t_1 = (t_0 ** 4)\n",
    "    t_2 = (W).dot((W.T).dot(u))\n",
    "    t_3 = (u).dot(t_2)\n",
    "    t_4 = (S).dot(u)\n",
    "    t_5 = (1 / t_1)\n",
    "    t_6 = (u).dot(t_4)\n",
    "    t_7 = t_4\n",
    "    t_8 = (4 / (t_0 ** 5))\n",
    "    t_9 = (u).dot(t_7)\n",
    "    t_10 = (t_3 * u)\n",
    "    functionValue = ((t_6 * t_3) / t_1)\n",
    "    gradient = ((((((t_5 * t_3) * t_4) + (t_5 * (t_3 * t_7))) + ((t_5 * t_6) * t_2)) + (t_5 * (t_9 * t_2))) - (((t_8 * t_6) * t_10) + ((t_8 * t_9) * t_10)))\n",
    "    return gradient\n",
    "\n",
    "def calculate_components(H,cov_H,W,n=2):\n",
    "    comps = []\n",
    "    H_proj = H.detach().cpu().numpy()\n",
    "    cov = cov_H.detach().cpu().numpy()\n",
    "    V = np.linalg.cholesky(cov)\n",
    "    W = W.detach().cpu().numpy()\n",
    "    P_nullspace = np.eye(768)\n",
    "    \n",
    "    for i in range(n):\n",
    "        print(\"Iteration {}\".format(i))\n",
    "        print(\"=========================\")\n",
    "        u = 0.1*np.random.randn(768)\n",
    "        u_last = u.copy()\n",
    "        \n",
    "        for j in range(100):\n",
    "            u_last, u = u, f(u,W,V,comps)\n",
    "            diff = np.linalg.norm(u_last-u)\n",
    "            if diff < 1e-5: break\n",
    "            if j%10 == 0: print(diff)\n",
    "        \n",
    "        comps.append(u/np.linalg.norm(u))\n",
    "\n",
    "        print(\"Start test.\")\n",
    "        \n",
    "        for k in range(len(comps)):\n",
    "            for t in range(len(comps)):\n",
    "                if k <= t: continue\n",
    "                print(\"Test\", comps[k].T@comps[t])\n",
    "                \n",
    "        print(\"Done test.\")\n",
    "        \n",
    "        #P_nullspace = np.eye(768) - u@u.T\n",
    "        #H_proj = H_proj@P_nullspace\n",
    "        #cov = np.cov(H_proj, rowvar = False)\n",
    "        #V = np.linalg.cholesky(cov)\n",
    "    \n",
    "    return comps \n",
    "\n",
    "comps = calculate_components(H,cov_H,W,n=2)\n",
    "\n",
    "# u = np.random.randn(768,1)\n",
    "# u_last = u.copy()\n",
    "\n",
    "# i = 0\n",
    "# while i < 100:\n",
    "#     u_last, u = u, f(u)\n",
    "#     print(np.linalg.norm(u_last-u))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999999993907"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps[0].T@comps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for i in range(len(bca)):\n",
    "    \n",
    "    #print(bca[i][\"explained_var\"])\n",
    "    \n",
    "#cov_out_total = get_cov_output_total(H,W)\n",
    "#total_var = get_loss_func(cov_out_total).detach().cpu().numpy().item()    \n",
    "vars = [x[\"explained_var\"] for x in bca]\n",
    "print(vars)\n",
    "print(sum(vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects  --------  There are also many social and political aspects that affect this system of marriage , throughout different societies and species . --------- -16.814509578906517\n",
      "======================================\n",
      "provisions  --------  These provisions , however , only apply to public bills that originate in the House of Commons . --------- -16.602402157293543\n",
      "======================================\n",
      "diaries  --------  Many individuals and some companies and groups use web logs or blogs , which are largely used as easily updatable online diaries . --------- -16.24923505438625\n",
      "======================================\n",
      "enemies  --------  Susan had hidden knowledge of the child from Dresden for the child 's safety not just because of his list of enemies , but hers as well . --------- -16.073949970223463\n",
      "======================================\n",
      "theory  --------  Hoxhaists also argue that the theory of New Democracy and People 's War were revisionist and anti-scientific . --------- -16.05632798176108\n",
      "======================================\n",
      "openings  --------  Several small openings were made in the structure for the positioning of bows and arrows , which can be seen even today . --------- -15.977961944754263\n",
      "======================================\n",
      "products  --------  From what I have read , the Federal Reserve controls the interest rates on financial products offered by banking institutions in order to promote a stable economy . --------- -15.36490248472846\n",
      "======================================\n",
      "gesturing  --------  Some programs have gradually integrated real-time controllers and gesturing ( for example , MIDI-driven software synthesis and parameter control ) . --------- -15.305884070562223\n",
      "======================================\n",
      "traditions  --------  While the stories are purely fictitious , they explore some extant beliefs and real-life traditions . --------- -15.293064906656912\n",
      "======================================\n",
      "problems  --------  The Island School pioneered the vision for the Putting Schools to Work Movement that focuses on teaching leadership by allowing students to tackle real world problems and issues through research , project planning and implementation . --------- -15.130031558756661\n",
      "======================================\n",
      "responses  --------  For each clue , Watson 's three most probable responses were displayed on the television screen . --------- -15.09537766122073\n",
      "======================================\n",
      "methods  --------  Usual methods include boiling , filtration , and chemical disinfection ( iodine or chlorine pills ) . --------- -15.060140323730977\n",
      "======================================\n",
      "lines  --------  New lines were added , including walk-behind vacuum sweepers , small parts organizers , debris deposal containers , and promotional and awareness training materials such as posters , decals and downloadable PowerPoint training presentations . --------- -14.950623060495417\n",
      "======================================\n",
      "drives  --------  It supports data compression and encryption , and parallel backups to multiple tape drives at the same time . --------- -14.891218067974126\n",
      "======================================\n",
      "goods  --------  In traditional economic models , inflation becomes more of a risk when the economy is closer to its capacity , because consumers are demanding more goods and services relative to supply , bidding up prices . --------- -14.847728187721657\n",
      "======================================\n",
      "processes  --------  Such a label undermines the multitudes of processes in occurrence and limits the extent of archaeological interpretation . --------- -14.782780393298935\n",
      "======================================\n",
      "initiatives  --------  Edna Gladney led two major initiatives resulting in significant changes to adoption practices . --------- -14.764135484390334\n",
      "======================================\n",
      "views  --------  These views may be of geographical features , man-made features such as roads and buildings or abstract representations of demographic quantities such as population . --------- -14.73308431359367\n",
      "======================================\n",
      "results  --------  PRIMARY ELECTIONDetailed information on measures and official results available from the Oregon Secretary of State . --------- -14.637048335369165\n",
      "======================================\n",
      "comments  --------  I would hope the comments here are enough to prevent that from happening . --------- -14.624982410314635\n",
      "======================================\n",
      "choices  --------  The aesthetic choices of a person create class fractions ( class-based social groups ) and actively distance one social class from the other social classes of a society . --------- -14.614516137978875\n",
      "======================================\n",
      "dynamics  --------  Keep Austin Weird '' moves beyond a mere slogan , to reflect the dynamics that encompass Austin . --------- -14.59854516862773\n",
      "======================================\n",
      "subscribers  --------  One frequent perceived technical problem with FeedBurner is the reduced number of subscribers being reported for the blogs using the service . --------- -14.581002463499837\n",
      "======================================\n",
      "debates  --------  There are debates , presentations and forums to help foster international understanding and communication . --------- -14.515173264100483\n",
      "======================================\n",
      "units  --------  A large orchestra of 109 players is divided into three orchestral units , each with its own conductor , which are deployed in a horseshoe shape to the left , front , and right of the audience . --------- -14.494938676435565\n",
      "======================================\n",
      "statements  --------  Patterson , issued an unusually strong statement saying Pakistan had witnessed an increase in `` provocative statements that promote intolerance and are an incitement to extremist violence . --------- -14.484936449043111\n",
      "======================================\n",
      "deoxynucleotides  --------  The DNA was composed of four nucleotides ( deoxyadenosine , deoxycytidine , deoxythymidine and deoxyguanosine ) , to the exclusion of other possible deoxynucleotides . --------- -14.423833498503503\n",
      "======================================\n",
      "theorists  --------  Synthethizing the arguments of multiple theorists is a gross violation of that dictum . --------- -14.418566459756615\n",
      "======================================\n",
      "words  --------  I could use words that are a lot stronger than what I did about Woodzing misrepresenting the position of Svanberg . --------- -14.39809655107975\n",
      "======================================\n",
      "research  --------  Many flight scientific research such as those investigating aerosol concentrations in the atmosphere and the Brown Cloud phenomenon have been initiated from Hanimaadhoo . --------- -14.348346231799784\n",
      "======================================\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "as  --------  He served in the Legislative Assembly of Manitoba as a Liberal-Progressive from 1949 to 1958 .\n",
      "======================================\n",
      "'m  --------  I 'm not the world 's leading expert on this but I believe this 13-years-old thing only applies if the site wishes to collect personal information from the user ( name , age , location , etc ) .\n",
      "======================================\n",
      "for  --------  In 1659 , Clayton was elected Member of Parliament for Lostwithiel in the Third Protectorate Parliament .\n",
      "======================================\n",
      "American  --------  His screenplay also tied with the ones for Being John Malkovich and American Beauty as the best of the year .\n",
      "======================================\n",
      "for  --------  Kelly made his debut for Cheshire in the 1958 Minor Counties Championship against the Yorkshire Second XI .\n",
      "======================================\n",
      "for  --------  From 1992 to 1997 , he represented the team in 6 Minor Counties fixtures , with his final appearance for the team coming against Cheshire .\n",
      "======================================\n",
      "film  --------  The Cowboy and the Lady is a 1938 American western romantic comedy film directed by H .\n",
      "======================================\n",
      "United  --------  The National Committee on American Foreign Policy ( NCAFP ) is an American nonprofit , nonpartisan activist organization dedicated to the resolution of conflicts that threaten United States interests .\n",
      "======================================\n",
      "for  --------  At the 1922 general election he was elected to the House of Commons as the Member of Parliament ( MP ) for the Gorbals division of Glasgow .\n",
      "======================================\n",
      "per  --------  As per 2001 census , Mathurapur II block had a total population of 198 , 261 , out of which 102 , 937 were males and 95 , 324 were females .\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "u = comps[0] #bca[0][\"vec\"]\n",
    "#u = np.expand_dims(u, axis=1)\n",
    "vecs = H.detach().cpu().numpy()\n",
    "vecs_u = vecs@u\n",
    "idx = np.argsort(vecs_u)\n",
    "k = 250\n",
    "top_neg, top_pos = idx[:k], idx[-k:]\n",
    "\n",
    "sents_pos = sents[top_pos]\n",
    "sents_neg = sents[top_neg]\n",
    "gold_pos = masked_tokens[top_pos]\n",
    "gold_neg = masked_tokens[top_neg]\n",
    "preds_pos = top_words[top_pos]\n",
    "preds_neg = top_words[top_neg]\n",
    "\n",
    "for i in range(30):\n",
    "    print(gold_pos[i], \" -------- \", sents_pos[i], \"---------\", vecs_u[idx[i]])\n",
    "    print(\"======================================\")\n",
    "    \n",
    "print(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(gold_neg[i], \" -------- \", sents_neg[i])\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(vecs_u)\n",
    "vecs_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bca[-1][\"cov_out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bca[-3][\"explained_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,v = np.random.randn(768), np.random.randn(768).T\n",
    "u,v = u / np.linalg.norm(u), v/np.linalg.norm(v)\n",
    "u@v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-12:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(768,32000)\n",
    "H = np.random.rand(10000,768)\n",
    "H*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(768,32000)\n",
    "H = torch.randn(10000, 768)\n",
    "Y_hat = H[:10000]@W \n",
    "torch.mean(Y_hat*Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_first_comp = get_first_pca(H)\n",
    "Y = H[:10000]@W\n",
    "Y_first_comp = get_first_pca(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_var = eval_total_var(H,W)\n",
    "print(total_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_var.sum()/W.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.randn(768,1)\n",
    "get_loss_func2(H_first_comp, cov_H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5%1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
